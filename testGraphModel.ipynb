{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbpY__f-Z9pH"
      },
      "outputs": [],
      "source": [
        "# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NLBrENLXjMt"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import os\n",
        "import torch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import global_max_pool,global_add_pool,global_mean_pool,GINEConv\n",
        "from torch_geometric.nn.norm import BatchNorm\n",
        "from torch.nn import Linear,Sequential,ReLU\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class PDBData(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_list, transform=None):\n",
        "        self.data_list = data_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = self.data_list[index]\n",
        "        if self.transform is not None:\n",
        "            data = self.transform(data)\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "def parseIndexFile(indexFilePath):\n",
        "    with open(indexFilePath, \"r\") as index_file:\n",
        "            pdbIDs = []\n",
        "            logKvalues = {}\n",
        "            for line in index_file:\n",
        "                if not line.startswith('#') and line.split()[4].startswith((\"Kd=\",\"Ki=\")):\n",
        "                    pdbIDs.append(str(line.split()[0]))\n",
        "                    logKvalues[str(line.split()[0])] = float(line.split()[3])\n",
        "    return pdbIDs, logKvalues\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GINENet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GINENet, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        dim = 64\n",
        "        self.atomicNumEmb = nn.Embedding(10,5)\n",
        "        self.formalChargeEmb = nn.Embedding(12,6)\n",
        "        self.aromaticEmb = nn.Embedding(2,1)\n",
        "        self.valenceEmb = nn.Embedding(9,4)\n",
        "        self.hybridizationEmb = nn.Embedding(6,3)\n",
        "        self.chiralityEmb = nn.Embedding(6,3)\n",
        "        self.numHEmb = nn.Embedding(10,5)\n",
        "        self.degreeEmb = nn.Embedding(12,6)\n",
        "        self.typeEmb = nn.Embedding(2,1)\n",
        "        self.residueEmb = nn.Embedding(31, 14)\n",
        "\n",
        "        self.bondTypeEmb = nn.Embedding(7,3)\n",
        "        self.bondDirEmb = nn.Embedding(5,2)\n",
        "        self.stereoEmb = nn.Embedding(6,3)\n",
        "        self.inRingEmb = nn.Embedding(2,1)\n",
        "        self.innerEmb = nn.Embedding(2,1)\n",
        "\n",
        "        self.conv1 = GINEConv(Sequential(Linear(49, dim), BatchNorm(dim), ReLU(),\n",
        "                                         Linear(dim, dim), ReLU()),\n",
        "                                         edge_dim = 11)\n",
        "\n",
        "        self.conv2 = GINEConv(Sequential(Linear(dim, dim), BatchNorm(dim), ReLU(),\n",
        "                                         Linear(dim, dim), ReLU()),\n",
        "                                         edge_dim = 11)\n",
        "\n",
        "        self.norm1 = BatchNorm(dim)\n",
        "        self.norm2 = BatchNorm(dim)\n",
        "\n",
        "        l1_size = 512\n",
        "        input_size = 2*dim*2\n",
        "        self.mlp = Sequential(Linear(input_size,l1_size),\n",
        "                                     BatchNorm(l1_size),\n",
        "                                     ReLU(),\n",
        "                                     Linear(l1_size, l1_size),\n",
        "                                     BatchNorm(l1_size),\n",
        "                                     ReLU(),\n",
        "                                     Linear(l1_size, l1_size),\n",
        "                                     BatchNorm(l1_size),\n",
        "                                     ReLU(),\n",
        "                                     Linear(l1_size,1))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "\n",
        "        atomicNums = self.atomicNumEmb(x[:,0].long())\n",
        "        chirality = self.chiralityEmb(x[:,1].long())\n",
        "        formalCharge = self.formalChargeEmb(x[:,2].long())\n",
        "        hybridizations = self.hybridizationEmb(x[:,4].long())\n",
        "        numHs = self.numHEmb(x[:,5].long())\n",
        "        valences = self.valenceEmb(x[:,5].long())\n",
        "        degrees = self.degreeEmb(x[:,6].long())\n",
        "        aromatics = self.aromaticEmb(x[:,7].long())\n",
        "        types = self.typeEmb(x[:,8].long())\n",
        "        mass = x[:,9].view(-1,1)\n",
        "        residues = self.residueEmb(x[:,10].long())\n",
        "\n",
        "\n",
        "        dists = edge_attr[:,0].view(-1,1)\n",
        "        bondTypes = self.bondTypeEmb(edge_attr[:,1].long())\n",
        "        bondDirs = self.bondDirEmb(edge_attr[:,2].long())\n",
        "        stereo = self.stereoEmb(edge_attr[:,3].long())\n",
        "        inRing = self.inRingEmb(edge_attr[:,4].long())\n",
        "        inner = self.innerEmb(edge_attr[:,5].long())\n",
        "\n",
        "        x = torch.cat((atomicNums,chirality,formalCharge,hybridizations,numHs,valences,degrees,aromatics,types,mass, residues),dim=1)\n",
        "        edge_attr = torch.cat((dists, bondTypes, bondDirs, stereo, inRing, inner), dim=1)\n",
        "        x1 = self.conv1(x, edge_index, edge_attr)\n",
        "        x1 = self.norm1(x1)\n",
        "        x2 = self.conv2(x1, edge_index, edge_attr)\n",
        "        x2 = self.norm2(x2)\n",
        "\n",
        "        x = torch.cat((x1,x2), dim=1)\n",
        "        x = torch.cat((global_add_pool(x,batch),global_max_pool(x,batch)),dim=1)\n",
        "        x  = self.mlp(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def test(loader,model):\n",
        "     model.eval()\n",
        "     predictions = []\n",
        "     for data in loader:\n",
        "         data.to(device)\n",
        "         out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "         predictions.append(out)\n",
        "     return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = GINENet()\n",
        "modelPath = \"\" #path to trained model\n",
        "model.load_state_dict(torch.load(modelPath,map_location=torch.device('cpu')))\n",
        "\n",
        "testSetPath = \"\" #folder with graphs of the testset\n",
        "testData = []\n",
        "files = os.listdir(testSetPath)\n",
        "print(files)\n",
        "for file in files:\n",
        "    graph = torch.load(f\"{testSetPath}/{file}\")\n",
        "    testData.append(graph)\n",
        "testloader = DataLoader(testData, batch_size=1, shuffle=False)\n",
        "predictions = test(testloader)\n",
        "predictions = [pred.item() for pred in predictions]\n",
        "print(predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
