{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPE3zapH4APS"
      },
      "outputs": [],
      "source": [
        "indexFullSet = \"INDEX_general_PL_data.2020\" #set to paths to your index files from PDBbind database\n",
        "indexRefinedSet = \"INDEX_refined_data.2020\"\n",
        "indexCoreSet = \"CoreSet.dat\"\n",
        "dataDir = \"PLECS_65536\" #  path to folder containing plec fingerprints in \".npy\" format with the pdbID as the name\n",
        "pdbInfoPath = \"PDBinfo.txt\"\n",
        "kinaseFilter = False # change to True for training without kinases in the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TngCoilL4AY3"
      },
      "outputs": [],
      "source": [
        "def parseIndexFile(indexFilePath):\n",
        "    with open(indexFilePath, \"r\") as index_file:\n",
        "            pdbIDs = []\n",
        "            logKvalues = {}\n",
        "            for line in index_file:\n",
        "                if line.startswith('#'):\n",
        "                  continue\n",
        "                if line.split()[4].startswith(('Ki=','Kd=')): #remove if training on full set with IC50 data\n",
        "                  pdbIDs.append(str(line.split()[0]))\n",
        "                  logKvalues[str(line.split()[0])] = float(line.split()[3])\n",
        "    return pdbIDs, logKvalues\n",
        "\n",
        "def filterKinases(pdbInfo):\n",
        "  kinases = []\n",
        "  with open(pdbInfo) as file:\n",
        "    for line in file.readlines():\n",
        "      pdbID = line.split(\"\\t\")[0]\n",
        "      pdbinfo = line.split(\"\\t\")[1]\n",
        "      if \"kinase\" in pdbinfo.lower() and pdbID not in kinases:\n",
        "        kinases.append(pdbID)\n",
        "  return kinases\n",
        "\n",
        "refinedIndex,logK = parseIndexFile(indexRefinedSet)\n",
        "coreIndex,logKcore = parseIndexFile(indexCoreSet)\n",
        "fullIndex,logKFull = parseIndexFile(indexFullSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwNOHzUo4Ado"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import callbacks\n",
        "seed = 46\n",
        "plec_size = 65536 #change to plec size 16384 for small plecs\n",
        "model_path = f\"mlp_model_{plec_size}\" # name of the saved model file\n",
        "random.seed(seed)\n",
        "\n",
        "trainData = []\n",
        "testData = []\n",
        "trainValues = []\n",
        "testValues = []\n",
        "testIDs = []\n",
        "trainIDs = []\n",
        "\n",
        "\n",
        "if not kinaseFilter:\n",
        "    for pdbID in fullIndex:\n",
        "        if not os.path.isfile(f\"{dataDir}/{pdbID}.npy\"):\n",
        "            continue\n",
        "        plec = np.load(f\"{dataDir}/{pdbID}.npy\")\n",
        "        if pdbID in coreIndex:\n",
        "            testData.append(plec)\n",
        "            testValues.append(logKFull[pdbID])\n",
        "            testIDs.append(pdbID)\n",
        "        elif pdbID in refinedIndex: # change to fullIndex for training on the filtered general set\n",
        "            trainData.append(plec)\n",
        "            trainValues.append(logKFull[pdbID])\n",
        "            trainIDs.append(pdbID)\n",
        "\n",
        "else:\n",
        "    kinases = filterKinases(pdbInfoPath)\n",
        "    for pdbID in refinedIndex:\n",
        "        if not os.path.isfile(f\"{dataDir}/{pdbID}.npy\"):\n",
        "            continue\n",
        "        plec = np.load(f\"{dataDir}/{pdbID}.npy\")\n",
        "        if pdbID in kinases:\n",
        "            testData.append(plec)\n",
        "            testValues.append(logKFull[pdbID])\n",
        "            testIDs.append(pdbID)\n",
        "        elif pdbID in refinedIndex and pdbID not in coreIndex:\n",
        "            trainData.append(plec)\n",
        "            trainValues.append(logKFull[pdbID])\n",
        "            trainIDs.append(pdbID)\n",
        "\n",
        "data = [(plec,val) for plec,val in zip(trainData,trainValues)]\n",
        "random.shuffle(data)\n",
        "trainData = [d[0] for d in data]\n",
        "trainValues = [d[1] for d in data]\n",
        "splitSize = int(len(trainData)/10)\n",
        "\n",
        "validData = np.array(trainData[:splitSize])\n",
        "validValues = np.array(trainValues[:splitSize])\n",
        "\n",
        "trainData = np.array(trainData[splitSize:])\n",
        "trainValues = np.array(trainValues[splitSize:])\n",
        "testData = np.array(testData)\n",
        "testValues = np.array(testValues)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl6U8Grm4AgU"
      },
      "outputs": [],
      "source": [
        "size = 200\n",
        "def neuralNet():\n",
        "  model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(size, activation='relu'),\n",
        "  tf.keras.layers.Dense(size, activation='relu'),\n",
        "  tf.keras.layers.Dense(size, activation='relu'),\n",
        "  tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n",
        "  return model\n",
        "\n",
        "model = neuralNet()\n",
        "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode = \"min\", patience = 50, restore_best_weights=True)\n",
        "\n",
        "model.fit(trainData,trainValues,epochs = 400, batch_size = 50, validation_data = (validData,validValues), shuffle=True, callbacks = [earlystopping])\n",
        "\n",
        "model.save(model_path)\n",
        "\n",
        "\n",
        "def statistics(data,values):\n",
        "  predictions = model.predict(data)\n",
        "  predictions = [float(val) for val in predictions]\n",
        "  mseLoss = tf.keras.losses.MeanSquaredError()\n",
        "  rmse = np.sqrt(mseLoss(predictions, values).numpy())\n",
        "  pearson = np.corrcoef(predictions, values)[0][1]\n",
        "  return rmse, pearson, predictions\n",
        "\n",
        "\n",
        "rmse, pearson, predictions = statistics(testData,testValues)\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"Pearson's r: {pearson}\")\n",
        "print(predictions)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
